Federated Learning for Human Activity Recognition (UCI HAR)

Privacy-Preserving Â· High-Accuracy Â· Lightweight PyTorch Pipeline
Overview
Welcome to a robust, privacy-preserving federated learning system for the UCI Human Activity Recognition (HAR) dataset.
Achieve state-of-the-art accuracy while protecting user privacy with differential privacy (DP) applied to model deltas.
All steps are automated and modular, with a lightweight PyTorch backbone.

Project Structure
.
â”œâ”€â”€ data/                   # Raw and processed data
â”‚   â””â”€â”€ raw/UCI_HAR/        # UCI HAR dataset (auto-downloaded)
â”œâ”€â”€ results/                # Model checkpoints, logs, tables, figures
â”œâ”€â”€ scripts/                # Pipeline scripts (preprocess, train, validate, evaluate)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/             # Training and FL configuration
â”‚   â”œâ”€â”€ datasets/           # Data loading and preprocessing
â”‚   â”œâ”€â”€ models/             # Model architectures (FNN)
â”‚   â”œâ”€â”€ federated/          # Aggregation logic
â”‚   â”œâ”€â”€ privacy/            # Differential privacy utilities
â”‚   â””â”€â”€ evaluation/         # Metrics and analysis
â””â”€â”€ run.sh                  # Unified pipeline script


Quickstart
git clone https://github.com/Cypher9802/FederatedLearning_Project.git
cd FederatedLearning_Project
bash run.sh

Everything is automated:
Dataset download, environment setup, preprocessing, training, privacy, and evaluation.


Key Features
Automated End-to-End Pipeline:
One command runs the entire workflow.
Differential Privacy on Deltas:
Industry-standard privacy with minimal accuracy loss.
High Accuracy:
Up to 93% (no privacy) and 91.5â€“91.7% (strong privacy).
Configurable:
Easily tune rounds, epochs, clients, DP parameters.

Accomplishments
1. Data Pipeline
ğŸ“¥ Automated UCI HAR download and setup.
ğŸ”¬ Feature normalization to `` with strict clipping.
ğŸ§‘â€ğŸ¤â€ğŸ§‘ Data split by subject to simulate federated clients.
âœ… Integrity and normalization checks.
2. Model
ğŸ¤– Feed-Forward Neural Network (FNN):
3 hidden layers (128, 64, 32), ReLU, dropout, modular and extensible.
3. Federated Learning Engine
ğŸ¢ Each subject = one client.
ğŸ”„ FedAvg aggregation, supports weighted averaging.
âš™ï¸ All parameters centralized for easy tuning.
4. Differential Privacy
ğŸ”’ DP on model deltas (client_model - global_model).
ğŸ›¡ï¸ Configurable noise_multiplier and clip_norm.
ğŸ“ˆ Ablation studies for privacy-utility tradeoff.
5. Evaluation & Reporting
ğŸ“ˆ Automated accuracy, F1, recall computation.
ğŸ“‘ Results saved in results/ for reproducibility.
ğŸ“Š Ablation and benchmarking scripts/tables included.
